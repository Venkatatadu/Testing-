import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import urljoin, urlparse
import time
import random
import logging
from requests.exceptions import RequestException
from fake_useragent import UserAgent

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class SecureWebScraper:
    def __init__(self, base_url, max_pages=5):
        self.base_url = base_url
        self.max_pages = max_pages
        self.visited_urls = set()
        self.user_agent = UserAgent()
        self.session = requests.Session()

    def get_random_delay(self):
        return random.uniform(1, 3)

    def is_valid_url(self, url):
        parsed = urlparse(url)
        return bool(parsed.netloc) and bool(parsed.scheme)

    def get_page(self, url):
        if not self.is_valid_url(url):
            logging.warning(f"Invalid URL: {url}")
            return None

        if url in self.visited_urls:
            logging.info(f"Already visited: {url}")
            return None

        try:
            headers = {'User-Agent': self.user_agent.random}
            response = self.session.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            self.visited_urls.add(url)
            return response.text
        except RequestException as e:
            logging.error(f"Error fetching {url}: {str(e)}")
            return None

    def parse_page(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        title = soup.title.string if soup.title else "No title found"
        paragraphs = [p.text.strip() for p in soup.find_all('p') if p.text.strip()]
        links = [urljoin(self.base_url, a['href']) for a in soup.find_all('a', href=True)]
        return {'title': title, 'paragraphs': paragraphs, 'links': links}

    def scrape(self):
        queue = [self.base_url]
        results = []

        while queue and len(results) < self.max_pages:
            url = queue.pop(0)
            html = self.get_page(url)
            if html:
                data = self.parse_page(html)
                results.append({'url': url, **data})
                queue.extend([link for link in data['links'] if link not in self.visited_urls])
                time.sleep(self.get_random_delay())

        return results

    def save_to_csv(self, data, filename):
        with open(filename, 'w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            writer.writerow(['URL', 'Title', 'Paragraph Count', 'Link Count'])
            for item in data:
                writer.writerow([
                    item['url'],
                    item['title'],
                    len(item['paragraphs']),
                    len(item['links'])
                ])
            logging.info(f"Data saved to {filename}")

def main():
    base_url = input("Enter the base URL to scrape: ")
    max_pages = int(input("Enter the maximum number of pages to scrape: "))
    
    scraper = SecureWebScraper(base_url, max_pages)
    results = scraper.scrape()
    
    if results:
        filename = f"{urlparse(base_url).netloc}_scrape_results.csv"
        scraper.save_to_csv(results, filename)
        print(f"Scraping completed. Results saved to {filename}")
    else:
        print("Scraping failed. No data to save.")

if __name__ == "__main__":
    main()
